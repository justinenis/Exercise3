---
title: "Exercise 3"
output: pdf_document
---

```{r, echo=FALSE, include=FALSE}
#imports libraries
library(ggplot2)
library(tidyverse)
library(knitr)
library(gtsummary)
library(rsample)
library(caret)
library(foreach)
library(modelr)
library(parallel)
library(lubridate)
library(dplyr)
library(data.table)
library(ggrepel)
library(ggmap)
```

# Question 1.1
Taking the data from a handful of cities would only establish correlation and not causation. To derive a casual relationship between number of police and the amount of crime, there would needed to be data taken of one variable that is unrelated to the other variable. 

# Question 1.2
The researchers are UPenn were able to isolate the effect by using the Terror Alert System and data from Washington, D.C. Since Washington, D.C. is considered a high target for terrorism, as the threat level increased to orange on the Terror Alert System, more police officers are added around the city, regardless of crime. Researchers were then able to observe a causal relationship between police and crime.

At the 5% level of significance, daily crime decreases by 7.316 occurrences when the Terror Alert System is high, all else equal. When controlling for midday ridership, daily crime decreases by 6.046 occurrences, all else equal. Additionally, at the 1% level of significance, midday ridership increases by 1734.1% when the Terror Alert System is high. In all cases, the R^2 is close to zero, which would be expected since police officers were added as a result of a high terrorist threat, and not in response to crime. 


# Question 1.3
When controlling for Metro ridership, the researchers were attempting to capture the effect on tourism in Washington, D.C. when the terror threat level was orange so as to understand if there would be fewer victims in a terrorist attack. 


# Question 1.4
The model being estimated here is the dependent variable of daily total number of crimes regressed on two interaction variables and a log taken for midday ridership. The two interactions are both interacting with the variable High Alert, with District 1 referring to a dummy variable. 

All else equal, daily total number of crimes decreases by 2.621 occurrences in District 1 when the terror threat level is high. All else equal, daily total number of crime decreases by 0.571 occurrences in other districts when the terror threat level is high. All else equal, midday ridership increases 247.7% when the terror threat level is high. All else equal, daily total number of crimes decreases by 11.058 occurrences when the terror threat level is high.

The conclusion is that a high terror alert increases the number of police around the city, which subsequently also increases the number of Metro riders and decreases crime in all areas of Washington, D.C., most notably in District 1.



# Question 2


## I. Overview
The objective is to analyze a data set consisting of 7,894 commercial rental properties from across the United States and create a predictive model so as to determine the effect of green buildings on revenue per square foot per calendar year. Of the 7,894 properties, 685 properties are considered to be a green building. To best achieve this objective, variable selection was used to narrow the data to relevant buildings classified as green, while holding all over variables constant.

## II. Data and Model
The creation of an interaction variable between rent and leasing_rate was used to predict the dependent variable, Annual_revenue_per_sqft. This varibale controls for any variance between high and low rent buildings and their appropriate occupancy rates throughout the year. 

Buildings are classified as green through either the LEED or Energy Star certification. Within the data set, the dummy variable, green_rating, encompasses both certifications and was used to narrow the data to the relevant information. Of the resulting 685 properties considered to be green, 6 observations lacked necessary information, and were subsequently dropped from the data set, resulting in 679 total observations. 

The next step is to use principal component analysis (PCA) as a means to fit a linear regression model to predict annual revenue per sqft. The goal of PCA is to use the remaining variables and create new "variables", called principal components (PCs). These PCs are scaled linear combinations of the original variables, but are also uncorrelated. There are 18 variables that were used to create the appropriate PCs, which include gas and electrical costs, and age of the building.


## III. Results

```{r, echo=FALSE}
#imports data
greenbuildings = read.csv('/Users/franklinstudent/Desktop/GitHub/Exercise3/greenbuildings.csv')

#creates new variable annual_revenue_per_sqft and drops vairbales with missing info
greenbuildings = greenbuildings %>%
  mutate(Annual_revenue_per_sqft = Rent*leasing_rate)%>%
  filter(green_rating == 1) %>%
  drop_na()

#generates new data set with specified columns
new_green = greenbuildings %>%
  select(1:4, 7:11, 15:24)

#creates maxtrix and dependent variable
X = as.matrix(new_green[,1:18])
y = new_green[,19]

#runs principal component analysis
pc_greenbuildings = prcomp(X, scale=TRUE)
greendata = pc_greenbuildings$x[,1:18]
pcr1 = lm(y ~ greendata)
```

```{r, echo=FALSE}
#summary of stats for model created
summary(pcr1)
```


```{r, echo=FALSE}
#plots model using data set
plot(fitted(pcr1), y, ylab="Annual_revenue_per_sqft")
title(main="Figure 1: Green Building Revenue", col.sub="blue")
abline(b = 1, a = 0, col="red")
```

Using PCA, I fit a linear regression model to predict annual revenue per sqft. Figure 1 displays a positive association between annual revenue per sqft and all other variables for green buildings. Most observations lie between 1000 and 4000 along the x-axis, with a predicted value of 1000-4000 in annual revenue per sqft. There also appears to be several outliers above the line of regression. 





## IV. Conclusion

Based on the results presented, there appears to be a positive associated between the amount of annual revenue per sqft that a green building can generate. Due to a buildings green certification, decreased gas and electrical costs may have an attractive sales appeal for perspective tenants, which may lead to higher prices that a building is able to charge. Moreover, clusters of certified green buildings would have a negative effect on prices, as it is direct competition in the area. To summarize, it would be a wise business objective for a building to invest and seek green certification. 

\newpage

# Question 3

## I. Overview 
The objective is to analyze the data set collected by the census tract for residential housing in the state of California, and develop a predictive model that could accurately determine the median house value based on several features. This model could be helpful in determining a pattern of median house values in the state. 



## II. Data and Model
In this model, the variable medianHouseValue is the dependent variable and there are 8 independent variables, such as medianHouseIncome, housingMedianAge, and location, which is represented by longitude and latitude. 

Two variables, totalRooms and totalBedrooms, are total numbers of rooms and bedrooms in a municipality. To make a better predictive model, both variables have been divided by the number of households of the municipality, so as to find the averages in each, and subsequently standardized. 

The next step is to train, test, split the original data set, which then provides an appropriate testing data set for the predictive model. To create the predictive model, I decided to use forward selection, which cycles through each variable and interaction to find the best model to test the data.





```{r, echo=FALSE}
#Imports data
housing = read.csv('/Users/franklinstudent/Desktop/GitHub/Exercise3/CAhousing.csv')

#Averaged by households and standardized variables totalRooms and totalBedrooms
housing = housing %>%
  mutate(totalRooms = totalRooms/households) %>%
  mutate(totalBedrooms = totalBedrooms/households) %>%
  mutate(across(totalRooms, scale)) %>%
  mutate(across(totalBedrooms, scale))
```

## III. Results




```{r, echo=FALSE, include=FALSE}

#train, test, split
housing_split = initial_split(housing, prop = 0.8)
housing_train = training(housing_split)
housing_test = testing(housing_split)


#forward selection
lm0 = lm(medianHouseValue ~ 1, data = housing_train)
lm_forward = step(lm0, direction = 'forward',
                  scope=~(longitude + latitude + housingMedianAge + 
                            totalRooms+ totalBedrooms + population +households + medianIncome)^2)
```

```{r, echo=FALSE}
#summary stats for created model
summary(lm_forward)
```
The resulting model has many features and interactions, with a relatively high R^2. Many of the variables produced include interactions with either longitude or latitude, which would be expected as location is an important determinate of home values. 




```{r, echo=FALSE}
#using model to predict prices of orginal data and finding resultig residuals
new_housing = housing %>%
  mutate(predicted =  predict(lm_forward, housing, type="response"))%>%
  mutate(residuals = medianHouseValue - predicted)

#using data to find out-of-sample accuracy
rmse_lm_forward = rmse(lm_forward, housing_test)
```

### Out-of-sample accuracy
```{r, echo=FALSE}
#out-of-sample accuracy 
rmse_lm_forward
```





```{r, echo=FALSE, include=FALSE}
#imports map from google API
ggmap::register_google(key = "AIzaSyAA9IwVuC5w1x2QtoM6KLXchxGMaETnQnM")
```

```{r, echo=FALSE, include=FALSE}
#creates initial map of California 
p <- ggmap(get_googlemap(center = c(lon =  -119.417931, lat = 36.778259),
                         zoom = 6, scale = 2,
                         maptype ='terrain',
                         color = 'color'))


#mapping of original data collected
figure1 = p + geom_point(aes(x = longitude, y = latitude, color = medianHouseValue), 
                       data = housing, size = 0.5)+ xlab("Longitude") + ylab("Latitude") + ggtitle("Figure 1: Median House Values") +
  theme(plot.title = element_text(hjust = 0.5))
   


#mapping of predictive model
figure2 = p + geom_point(aes(x = longitude, y = latitude, color = medianHouseValue), 
                       data = housing_test, size = 0.5) + xlab("Longitude") + ylab("Latitude") + ggtitle("Figure 2: Predicted Median House Values") +
  theme(plot.title = element_text(hjust = 0.5))




#mapping of residuals between orginial data and predicted model
figure3 = p + geom_point(aes(x = longitude, y = latitude, color = residuals), 
                         data = new_housing, size = 0.5) + xlab("Longitude") + ylab("Latitude") + ggtitle("Figure 3: Residuals") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r, echo=FALSE}
#output of figure1 
figure1
```
Figure 1 displays the original data set using the respective coordinates to plot the points on a map of the state of California. Upon observation, the highest median house values are along the coast, particularly in the San Francisco and Los Angeles regions, with several high median house values in the Lake Tahoe area. In contrast, the lowest median house values are in the Central Valley of the state.


```{r, echo=FALSE}
#output of figure2
figure2
```
Figure 2 displays the predictive model of median house values using the original data set. As previously found in Figure 1, the data points display a clear indication of highest median house values along the California coast and several in the Lake Tahoe area, while the lowest median house values are in the Central Valley.




```{r, echo=FALSE}
#output of figure3
figure3
```
Figure 3 displays the residuals of the predictive model using the orginal data set. As expected, the map produced displays most data points centered around 0, with several outliers. This conveys that the created model was successful in determining the median house values. 


## IV. Conclusion

Using feature selection as a means to develop a model has been proven to be successful in predicting median house values across the state of California. Median house prices are highest along the coast and lowest in the Central Valley, which were expected and derived the original data set. The out-of-sample accuracy for the model is consistently close to 67000. Moreover, an obvious pattern to median house values has been detected and can be used for further analysis. 

